{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/nawaz/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nawaz/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nawaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'said', 'that', 's', 'it', 'u', 'Hello', 'World']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = '''He said,\"that's it.\" *u* Hello, World.'''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Worst', 'NNP'), ('bank', 'NN'), ('ever', 'RB')], [('Initially', 'RB'), ('I', 'PRP'), ('liked', 'VBD'), ('app', 'RB'), ('unfortunately', 'RB'), ('frame', 'JJ'), ('policies', 'NNS'), ('per', 'IN')], [('Virtual', 'JJ'), ('assistant', 'NN'), ('worst', 'JJS'), ('u', 'JJ'), ('ask', 'NN'), ('something', 'NN'), ('say', 'VBP'), ('something', 'NN'), ('else', 'RB')], [('Secondly', 'RB'), ('even', 'RB'), ('Standing', 'VBG'), ('instruction', 'NN'), ('account', 'NN'), ('bank', 'NN'), ('act', 'NN'), ('per', 'IN')], [('Worst', 'RBS')], []]\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Worst bank ever. Initially I liked this app but unfortunately they frame policies as per their will. \n",
    "Virtual assistant is worst, u ask something it will say something else! Secondly, \n",
    "even there is a Standing instruction to your account the bank will act per it's will. Worst!!\n",
    "'''\n",
    "tagged = []\n",
    "tokenized = sent_tokenize(text) \n",
    "for i in tokenized: \n",
    "    wordsList = tokenizer.tokenize(i) #nltk.word_tokenize(i) \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged.append(nltk.pos_tag(wordsList)) \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bank', 'NN')]\n",
      "[]\n",
      "[('assistant', 'NN'), ('ask', 'NN'), ('something', 'NN'), ('something', 'NN')]\n",
      "[('instruction', 'NN'), ('account', 'NN'), ('bank', 'NN'), ('act', 'NN')]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tagged:\n",
    "    print([i for i in sentence if i[1] == \"NN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import chunk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# tree = chunk.ne_chunk(tagged)\n",
    "# tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‘big’\n",
    "JJR adjective, comparative ‘bigger’\n",
    "JJS adjective, superlative ‘biggest’\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‘desk’\n",
    "NNS noun plural ‘desks’\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "PDT predeterminer ‘all the kids’\n",
    "POS possessive ending parent‘s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO to go ‘to‘ the store.\n",
    "UH interjection errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSComp",
   "language": "python",
   "name": "awscomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

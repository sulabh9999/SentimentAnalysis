{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Topic Modeling](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\\nstandford source: https://nlp.stanford.edu/software/CRF-NER.shtml\\nstandford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "# reload only imported modules before run\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "'''\n",
    "pantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "standford source: https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "standford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '01-11-2018' #  09-Sep-2018 \n",
    "end_date = '30-12-2018' # 01-Oct-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will perform the following steps:\n",
    "\n",
    "#### 1. Tokenization: \n",
    "Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove small words:\n",
    "Words that have fewer than 3 characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isShortWord(token):\n",
    "#     return len(token) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stopwords:\n",
    "All stopwords are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isStopWord(token):\n",
    "#     return token in gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. lemmatized +  Stemming:\n",
    "Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk import pos_tag, word_tokenize\n",
    "# from nltk.stem.porter import *\n",
    "# from textblob import TextBlob\n",
    "\n",
    "\n",
    "# def lemmatize_stemming(token):\n",
    "#     stemmer = PorterStemmer() #gensim.parsing.stem_text(tokenize) #\n",
    "#     for word, tag in pos_tag(word_tokenize(token)):\n",
    "#         wntag = tag[0].lower()\n",
    "#         wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "#         lemma = WordNetLemmatizer().lemmatize(word, wntag) if wntag else word\n",
    "#         return TextBlob(lemma).words[0].singularize()\n",
    "#     return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Replace Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get emoji characters file path\n",
    "# def getEmojis():\n",
    "#     from dataSource import getEmojis\n",
    "#     comments_file_path = getDataSourcePathFor(emoji_path)\n",
    "#     return getEmojis(comments_file_path)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hasEmojicon(token):\n",
    "    \n",
    "# def replaceEmojicons(token, emojies):\n",
    "#     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getNounList(sentence='', tokens = []):\n",
    "#     from nltk import word_tokenize, pos_tag\n",
    "#     if len(tokens) > 0:    \n",
    "#         nouns = [token for token, pos in pos_tag(tokens) if pos.startswith('NN')]\n",
    "#         return nouns\n",
    "#     else:\n",
    "#         nouns = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('NN')]\n",
    "#         return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'I Have done reviewing, Will be seeing by them'\n",
    "# print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is file storage path\n",
    "# def getDataSourcePathFor(keyForFilePath):\n",
    "#     import json\n",
    "#     import os\n",
    "    \n",
    "#     config_file_path = os.environ[virtual_env] + '/config.json'\n",
    "\n",
    "#     with open(config_file_path) as f:\n",
    "#         config = json.load(f)\n",
    "#         if keyForFilePath in config:# ['comments_path', 'output_path']\n",
    "#             return config[keyForFilePath] \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get list of comments from stored input csv file\n",
    "# import dataSource \n",
    "\n",
    "# def getListOfComments():\n",
    "#     ### This is to get csv rows between given dates\n",
    "#     comments_file_path = getDataSourcePathFor(comments_path)\n",
    "#     commentsList = getComments(comments_file_path, start_date, end_date) #['comments'] \n",
    "#     commentsList = commentsList.sort_values(by='ratings', ascending=True)['comments'] \n",
    "#     print('Total number of comments: %s between %s and %s' % (len(commentsList), start_date, end_date))\n",
    "#     return commentsList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def filterWord(token):\n",
    "#     if not (isStopWord(token) or isShortWord(token)):\n",
    "#         lemmaWord = lemmatize_stemming(token)\n",
    "#         if not isShortWord(lemmaWord):\n",
    "#             return (\"\".join(re.findall(\"[a-zA-Z]+\", lemmaWord)).lower())\n",
    "#     return None \n",
    "    \n",
    "# def filters(sentence):\n",
    "# #     print('..given comments:', sentence)\n",
    "#     result = []\n",
    "#     #nouns = getNounList(sentence) # fetch only Nouns\n",
    "#     for token in tokenize(sentence):#nouns: ###tokenize(text):\n",
    "#         result.append(filterWord(token))\n",
    "#     return result\n",
    "\n",
    "# def filterWords(tokens):\n",
    "#     return list(filter(lambda token: filterWord(token), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessCommentDocument(document):\n",
    "#     return list(map(lambda sentence: filters(sentence), document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordCloud import showWordCloud\n",
    "def showCloud(topicCollection):\n",
    "    showWordCloud(topicCollection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(processed_docs):\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    return bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getSentiment import getSentiment\n",
    "\n",
    "def getSentFromCommentList(commentList):\n",
    "    sentimentList = []\n",
    "    for sentence in commentList:\n",
    "        sentiment = getSentiment(sentence)\n",
    "        sentimentList.append(sentiment)\n",
    "    return sentimentList\n",
    "\n",
    "\n",
    "def isNegative(sentiment):\n",
    "    return sentiment > 0.5\n",
    "\n",
    "\n",
    "def tokenDictWithPosNegSentiment(sentimentList, document):\n",
    "    sentDict = {}\n",
    "    size = range(len(document))\n",
    "    \n",
    "    for i in size:\n",
    "        for token in document[i]:\n",
    "            v = (0, 0, 0) # (neg, pos, freq)\n",
    "            if token in sentDict:\n",
    "                v = sentDict[token]\n",
    "                \n",
    "            if isNegative(sentimentList[i]):\n",
    "                v = (v[0]+1, v[1], v[2]+1)\n",
    "            else:\n",
    "                v = (v[0], v[1]+1, v[2]+1)\n",
    "            sentDict[token] = v\n",
    "    sentDict.pop('', None)\n",
    "    return sentDict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SDataSource\n",
    "import SConstants\n",
    "from SWordList import SWordList\n",
    "from sUtility import SUtility\n",
    "from sPreprocessor import SPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments: 972 between 01-11-2018 and 30-12-2018\n"
     ]
    }
   ],
   "source": [
    "sutility = SUtility()\n",
    "spreprocessor = SPreprocessor()\n",
    "\n",
    "# SPreprocessor.resolveDependancy(trieCommon)\n",
    "dateBetween = [start_date, end_date]\n",
    "commentsDocument = SDataSource.getListOfComments(dateBetween).head(1000)\n",
    "\n",
    "# document preprocessing, cleaning, filtering, replacement, spliting into multiple senetnces from one\n",
    "processed_doc = []\n",
    "for sentence in commentsDocument:\n",
    "    for each in spreprocessor.docCleaning(sentence):\n",
    "        processed_doc.append(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(sentence):\n",
    "    t = spreprocessor.parseToTokens(trieTopic, sentence)\n",
    "    r = spreprocessor.parseToTokens(trieNReason, sentence)\n",
    "#     print(sentence)\n",
    "#     print('topic is:', t)\n",
    "#     print('reasons are:', r)\n",
    "#     print()\n",
    "    sutility.dump(t, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..SWordList constructor called \n",
      "..SWordList constructor called \n",
      "..SWordList constructor called \n"
     ]
    }
   ],
   "source": [
    "wordList_file = SDataSource.getDataSourcePathFor(SConstants.wordFile_path)\n",
    "trieCommon = SWordList(wordList_file)\n",
    "\n",
    "topic_file = SDataSource.getDataSourcePathFor(SConstants.topic_path)\n",
    "trieTopic = SWordList(topic_file)\n",
    "\n",
    "n_reason_file = SDataSource.getDataSourcePathFor(SConstants.n_reason_path)\n",
    "trieNReason = SWordList(n_reason_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crash', 'chang', 'upi', 'biometr']\n"
     ]
    }
   ],
   "source": [
    "l = ['crashing', 'changes', 'upi', 'biometric']\n",
    "print(spreprocessor.filterWords(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sNetworkx import SNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in processed_doc:\n",
    "    makeDict(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('app', 403), ('account', 120), ('login', 93), ('debit card', 71), ('updat', 61), ('upi', 42), ('version', 20), ('atm', 17), ('customer care', 16), ('kyc', 15), ('balanc', 13), ('customer service', 11), ('debit cards', 8), ('look', 7), ('offer', 6), (None, 6), ('speed', 6), ('secur', 5), ('internet', 4), ('biometr', 4), ('signup', 3), ('ifsc', 3), ('charg', 3), ('postal code', 1), ('international transactions', 1), ('deals and offers', 1), ('back option', 1), ('another bank', 1)]\n",
      "---------------------------------\n",
      "app\n",
      "[('open', 24), ('not working', 16), ('crash', 9), ('every time', 8), ('problem', 5), ('chang', 5), ('bad', 5), ('servic', 5), ('reset', 4), ('popup', 4), ('stop', 4), ('unstabl', 3), ('something went wrong', 3), ('frustrat', 3), ('transfer', 2), ('buggi', 2), ('screen', 2), ('suck', 2), ('stuck', 2), ('fix it', 1), ('incorrect', 1), ('broke', 1), ('worst experience', 1), ('reflect', 1), ('deposit', 1), ('forc', 1)]\n",
      "\n",
      "account\n",
      "[('open', 26), ('transfer', 6), ('servic', 6), ('problem', 4), ('close', 3), ('bad', 3), ('deduct', 2), ('stop', 2), ('reset', 2), ('screen', 2), ('inact', 2), ('fix it', 1), ('frustrat', 1), ('wrong', 1), ('not working', 1), ('popup', 1), ('reflect', 1), ('dormant', 1), ('deposit', 1), ('add amount', 1), ('chang', 1)]\n",
      "\n",
      "login\n",
      "[('reset', 9), ('every time', 5), ('problem', 3), ('face', 3), ('incorrect', 2), ('chang', 2), ('crash', 2), ('something is wrong', 1), ('frustrat', 1), ('open', 1), ('not working', 1), ('popup', 1), ('bad', 1), ('servic', 1), ('something wrong', 1), ('screen', 1), ('server issue', 1)]\n",
      "\n",
      "debit card\n",
      "[('popup', 3), ('problem', 3), ('bad', 1), ('transfer', 1), ('servic', 1), ('forc', 1), ('chang', 1), ('incorrect', 1)]\n",
      "\n",
      "updat\n",
      "[('not working', 3), ('popup', 3), ('crash', 3), ('chang', 2), ('every time', 2), ('stop', 2), ('open', 2), ('reset', 2), ('fix it', 1), ('suck', 1), ('face', 1), ('incorrect', 1), ('buggi', 1), ('worst experience', 1), ('broke', 1)]\n",
      "\n",
      "upi\n",
      "[('every time', 4), ('problem', 3), ('transfer', 1), ('chang', 1), ('open', 1), ('not working', 1), ('popup', 1), ('not fixed', 1), ('face', 1), ('add amount', 1), ('stuck', 1), ('screen', 1), ('close', 1), ('inact', 1)]\n",
      "\n",
      "version\n",
      "[('crash', 2), ('open', 1), ('stop', 1), ('suck', 1)]\n",
      "\n",
      "atm\n",
      "[('open', 1), ('incorrect', 1), ('servic', 1), ('problem', 1)]\n",
      "\n",
      "customer care\n",
      "[('servic', 3), ('bad', 1), ('open', 1)]\n",
      "\n",
      "kyc\n",
      "[('face', 1), ('popup', 1), ('transfer', 1), ('problem', 1), ('open', 1)]\n",
      "\n",
      "balanc\n",
      "[('open', 1), ('problem', 1), ('stop', 1), ('close', 1)]\n",
      "\n",
      "customer service\n",
      "[('servic', 11), ('problem', 2), ('worst experience', 1)]\n",
      "\n",
      "debit cards\n",
      "[]\n",
      "\n",
      "look\n",
      "[('not working', 1), ('crash', 1), ('suck', 1), ('chang', 1)]\n",
      "\n",
      "offer\n",
      "[('open', 1)]\n",
      "\n",
      "None\n",
      "[('servic', 1)]\n",
      "\n",
      "speed\n",
      "[]\n",
      "\n",
      "secur\n",
      "[('reset', 1), ('every time', 1)]\n",
      "\n",
      "internet\n",
      "[]\n",
      "\n",
      "biometr\n",
      "[('open', 1), ('bad', 1)]\n",
      "\n",
      "signup\n",
      "[]\n",
      "\n",
      "ifsc\n",
      "[('open', 1), ('reflect', 1)]\n",
      "\n",
      "charg\n",
      "[('servic', 2)]\n",
      "\n",
      "postal code\n",
      "[('chang', 1)]\n",
      "\n",
      "international transactions\n",
      "[]\n",
      "\n",
      "deals and offers\n",
      "[]\n",
      "\n",
      "back option\n",
      "[('not working', 1), ('suck', 1), ('chang', 1)]\n",
      "\n",
      "another bank\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sutility.showTopicCounts()\n",
    "print('---------------------------------')\n",
    "sutility.showReasonDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('app', 403), ('account', 120), ('login', 93), ('debit card', 71), ('updat', 61), ('upi', 42), ('version', 20), ('atm', 17), ('customer care', 16), ('kyc', 15), ('balanc', 13), ('customer service', 11), ('debit cards', 8), ('look', 7), ('offer', 6), (None, 6), ('speed', 6), ('secur', 5), ('internet', 4), ('biometr', 4), ('signup', 3), ('ifsc', 3), ('charg', 3), ('postal code', 1), ('international transactions', 1), ('deals and offers', 1), ('back option', 1), ('another bank', 1)]\n"
     ]
    }
   ],
   "source": [
    "sutility.showTopicCounts()\n",
    "# sentList = commentsDocument.map(getSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = list(commentsDocument)\n",
    "# p = list(processed_doc)\n",
    "\n",
    "# length = len(l)\n",
    "\n",
    "# for i in range(length):\n",
    "#     print(l[i])\n",
    "#     print(p[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not able to add amount from another upi acoount or also not able to transfer from another banks account it shows me benificary account is inactive or major problem not setup a upi id\n",
      "i am cant add amount from another upi acoount or also cant transfer from another banks account it shows me benificary account is inactive or major problem not setup a upi id\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-3b8055c69aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentList' is not defined"
     ]
    }
   ],
   "source": [
    "dump = list(zip(commentsDocument, processed_doc))\n",
    "length = len(dump)\n",
    "for i in range(length):\n",
    "    print(dump[i][0])\n",
    "    print(dump[i][1])\n",
    "    print(sentList.iloc[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_doc)\n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "corpus_tfidf = models.TfidfModel(bow_corpus)[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexFor = 5\n",
    "# print('..COMMENT :', commentsDocument[indexFor])\n",
    "# print('\\n..CORPOS :', processed_doc[indexFor])\n",
    "# # print('\\n..BOW:', bow_corpus[indexFor])\n",
    "# for index, score in sorted(lda_model_tfidf[bow_corpus[indexFor]], key=lambda tup: -1*tup[1]):\n",
    "#     print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_tfidf.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processed_doc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tokenDictWithPosNegSentiment(list(sentList), list(processed_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sortedMostPos(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from showBarGraph import *\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostPos(l), pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sortedMostNeg(l))\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostNeg(l), pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPiChart(sortedMostFreq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showTempBarChart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSComp",
   "language": "python",
   "name": "awscomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

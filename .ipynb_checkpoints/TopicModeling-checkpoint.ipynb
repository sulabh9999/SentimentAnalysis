{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Topic Modeling](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nawaz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "from gensim import parsing\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_env = 'VIRTUAL_ENV'\n",
    "comments_path = 'comments_path'\n",
    "emoji_path = 'emoji_path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will perform the following steps:\n",
    "\n",
    "#### 1. Tokenization: \n",
    "Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove small words:\n",
    "Words that have fewer than 3 characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isShortWord(token):\n",
    "    return len(token) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stopwords:\n",
    "All stopwords are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isStopWord(token):\n",
    "    return token in gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. lemmatized +  Stemming:\n",
    "Words are lemmatizedâ€Šâ€”â€Šwords in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "Words are stemmedâ€Šâ€”â€Šwords are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token):\n",
    "    stemmer = PorterStemmer() #gensim.parsing.stem_text(tokenize) #\n",
    "    for word, tag in pos_tag(word_tokenize(token)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        lemma = WordNetLemmatizer().lemmatize(word, wntag) if wntag else word\n",
    "        return lemma\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Replace Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get emoji characters file path\n",
    "# def getEmojis():\n",
    "#     from dataSource import getEmojis\n",
    "#     comments_file_path = getDataSourcePathFor(emoji_path)\n",
    "#     return getEmojis(comments_file_path)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hasEmojicon(token):\n",
    "    \n",
    "# def replaceEmojicons(token, emojies):\n",
    "#     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounList(sentence):\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    nouns = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('NN')]\n",
    "#     print('..pos tag:..', pos_tag(word_tokenize(sentence)))\n",
    "#     print('..Noun list is: ', nouns)\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'I Have done reviewing, Will be seeing by them'\n",
    "# print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is file storage path\n",
    "def getDataSourcePathFor(keyForFilePath):\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    config_file_path = os.environ[virtual_env] + '/config.json'\n",
    "\n",
    "    with open(config_file_path) as f:\n",
    "        config = json.load(f)\n",
    "        return config[keyForFilePath] # ['comments_path', 'output_path']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get list of comments from stored input csv file\n",
    "def getListOfComments():\n",
    "    from dataSource import getComments\n",
    "    ### This is to get csv rows between given dates\n",
    "    start_date = '27-09-2018'\n",
    "    end_date = '02-10-2018'\n",
    "    comments_file_path = getDataSourcePathFor(comments_path)\n",
    "    return getComments(comments_file_path, start_date, end_date)['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filters(sentence):\n",
    "    print('..given comments:', sentence)\n",
    "    result = []\n",
    "    nouns = getNounList(sentence) # fetch only Nouns\n",
    "    for token in nouns: ###tokenize(text):\n",
    "        if not (isStopWord(token) or isShortWord(token)):\n",
    "            lemmaWord = lemmatize_stemming(token)\n",
    "            if not isShortWord(lemmaWord):\n",
    "                result.append(lemmaWord)\n",
    "    print('\\n..tokens:', result)\n",
    "    print('\\n\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCommentDocument(document):\n",
    "    return list(map(lambda sentence: filters(sentence), document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordCloud import showWordCloud\n",
    "def showColud():\n",
    "    showWordCloud(topicCollection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(processed_docs):\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    # TODO:- need to tune parameter if doc size is large\n",
    "    dictionary.filter_extremes(no_below=0, no_above=0.5, keep_n=100000)\n",
    "    print(dictionary)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    print(bow_corpus)\n",
    "    print(dictionary.token2id)\n",
    "#     for each in dictionary:\n",
    "#         print(each),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    commentsDocument = getListOfComments().head(2)\n",
    "    showCloud(commentsDocument)\n",
    "    processed_doc = preprocessCommentDocument(commentsDocument)\n",
    "    bow(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..given comments: Very good app and lots of features. Debit card is so easy to use and it's all free. It's monthly spendings tracker features is so awesome. Love it. Overall love this app and it's one of best in market. One thing I wish to have is credit card facilities for the Indian salaried persons.\n",
      "\n",
      "..tokens: ['app', 'lot', 'feature', 'Debit', 'card', 'spending', 'tracker', 'feature', 'love', 'app', 'market', 'thing', 'credit', 'card', 'facility', 'person']\n",
      "\n",
      "\n",
      "\n",
      "..given comments: Worst banking app. After tried for various times I am unable to log in to my account. God knows wht wll happen to my hard earned moneyðŸ˜ ðŸ˜ ðŸ¤¯\n",
      "\n",
      "..tokens: ['Worst', 'banking', 'app', 'time', 'account', 'God', 'happen', 'moneyðŸ˜ ðŸ˜ \\U0001f92f']\n",
      "\n",
      "\n",
      "\n",
      "Dictionary(19 unique tokens: ['account', 'feature', 'credit', 'tracker', 'God']...)\n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)]]\n",
      "{'account': 14, 'feature': 4, 'credit': 2, 'tracker': 11, 'God': 12, 'market': 7, 'love': 6, 'banking': 15, 'card': 1, 'lot': 5, 'Worst': 13, 'person': 8, 'Debit': 0, 'spending': 9, 'facility': 3, 'moneyðŸ˜ ðŸ˜ \\U0001f92f': 17, 'thing': 10, 'time': 18, 'happen': 16}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSComp",
   "language": "python",
   "name": "awscomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

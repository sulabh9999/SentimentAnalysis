{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Topic Modeling](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\\nstandford source: https://nlp.stanford.edu/software/CRF-NER.shtml\\nstandford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "from gensim import parsing\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "# reload only imported modules before run\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "'''\n",
    "pantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "standford source: https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "standford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_env = 'VIRTUAL_ENV'\n",
    "comments_path = 'comments_path'\n",
    "emoji_path = 'emoji_path'\n",
    "wordFile_path = 'wordFile_path'\n",
    "\n",
    "start_date = '09-01-2018' #  09-Sep-2018 \n",
    "end_date = '01-10-2018' # 01-Oct-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will perform the following steps:\n",
    "\n",
    "#### 1. Tokenization: \n",
    "Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove small words:\n",
    "Words that have fewer than 3 characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isShortWord(token):\n",
    "    return len(token) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stopwords:\n",
    "All stopwords are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isStopWord(token):\n",
    "    return token in gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. lemmatized +  Stemming:\n",
    "Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token):\n",
    "    stemmer = PorterStemmer() #gensim.parsing.stem_text(tokenize) #\n",
    "    for word, tag in pos_tag(word_tokenize(token)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        lemma = WordNetLemmatizer().lemmatize(word, wntag) if wntag else word\n",
    "        return TextBlob(lemma).words[0].singularize()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Replace Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get emoji characters file path\n",
    "# def getEmojis():\n",
    "#     from dataSource import getEmojis\n",
    "#     comments_file_path = getDataSourcePathFor(emoji_path)\n",
    "#     return getEmojis(comments_file_path)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hasEmojicon(token):\n",
    "    \n",
    "# def replaceEmojicons(token, emojies):\n",
    "#     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounList(sentence):\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    nouns = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('NN')]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'I Have done reviewing, Will be seeing by them'\n",
    "# print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is file storage path\n",
    "def getDataSourcePathFor(keyForFilePath):\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    config_file_path = os.environ[virtual_env] + '/config.json'\n",
    "\n",
    "    with open(config_file_path) as f:\n",
    "        config = json.load(f)\n",
    "        if keyForFilePath in config:# ['comments_path', 'output_path']\n",
    "            return config[keyForFilePath] \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get list of comments from stored input csv file\n",
    "from dataSource import getComments, sortedMostNeg, sortedMostPos, sortedMostFreq\n",
    "def getListOfComments():\n",
    "    ### This is to get csv rows between given dates\n",
    "    comments_file_path = getDataSourcePathFor(comments_path)\n",
    "    commentsList = getComments(comments_file_path, start_date, end_date)['comments'] \n",
    "    print('Total number of comments: %s between %s and %s' % (len(commentsList), start_date, end_date))\n",
    "    return commentsList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filters(sentence):\n",
    "#     print('..given comments:', sentence)\n",
    "    result = []\n",
    "    #nouns = getNounList(sentence) # fetch only Nouns\n",
    "    for token in tokenize(sentence):#nouns: ###tokenize(text):\n",
    "        if not (isStopWord(token) or isShortWord(token)):\n",
    "            lemmaWord = lemmatize_stemming(token)\n",
    "            if not isShortWord(lemmaWord):\n",
    "                result.append(\"\".join(re.findall(\"[a-zA-Z]+\", lemmaWord)).lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCommentDocument(document):\n",
    "    return list(map(lambda sentence: filters(sentence), document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordCloud import showWordCloud\n",
    "def showCloud(topicCollection):\n",
    "    showWordCloud(topicCollection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(processed_docs):\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getSentiment import getSentiment\n",
    "\n",
    "def getSentFromCommentList(commentList):\n",
    "    sentimentList = []\n",
    "    for sentence in commentList:\n",
    "        sentiment = getSentiment(sentence)\n",
    "        sentimentList.append(sentiment)\n",
    "    return sentimentList\n",
    "\n",
    "\n",
    "def isNegative(sentiment):\n",
    "    return sentiment > 0.5\n",
    "\n",
    "\n",
    "def tokenDictWithPosNegSentiment(sentimentList, document):\n",
    "    sentDict = {}\n",
    "    size = range(len(document))\n",
    "    \n",
    "    for i in size:\n",
    "        for token in document[i]:\n",
    "            v = (0, 0, 0) # (neg, pos, freq)\n",
    "            if token in sentDict:\n",
    "                v = sentDict[token]\n",
    "                \n",
    "            if isNegative(sentimentList[i]):\n",
    "                v = (v[0]+1, v[1], v[2]+1)\n",
    "            else:\n",
    "                v = (v[0], v[1]+1, v[2]+1)\n",
    "            sentDict[token] = v\n",
    "    sentDict.pop('', None)\n",
    "    return sentDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments: 880 between 09-01-2018 and 01-10-2018\n"
     ]
    }
   ],
   "source": [
    "commentsDocument = getListOfComments().head(5)\n",
    "processed_doc = preprocessCommentDocument(commentsDocument)\n",
    "# print('..bag of words: ', bow(processed_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentList = [] #getSentFromCommentList(commentsDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..DBSWordList constructor called\n"
     ]
    }
   ],
   "source": [
    "from dbsWordList import DBSWordList\n",
    "file_path = getDataSourcePathFor(wordFile_path)\n",
    "trie = DBSWordList(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_doc_exist_words = trie.searchInDocument(processed_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tokenDictWithPosNegSentiment(sentList, proc_doc_exist_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(sortedMostPos(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from showBarGraph import *\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostPos(l), pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sortedMostNeg(l))\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostNeg(l), pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPiChart(sortedMostFreq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTempBarChart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSComp",
   "language": "python",
   "name": "awscomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

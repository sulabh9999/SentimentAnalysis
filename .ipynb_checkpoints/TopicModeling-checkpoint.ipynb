{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Topic Modeling](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\\nstandford source: https://nlp.stanford.edu/software/CRF-NER.shtml\\nstandford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\\n'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "from gensim import parsing\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "# reload only imported modules before run\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "'''\n",
    "pantree bank tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "standford source: https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "standford online text tree generater: http://nlp.stanford.edu:8080/parser/index.jsp\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_env = 'VIRTUAL_ENV'\n",
    "comments_path = 'comments_path'\n",
    "emoji_path = 'emoji_path'\n",
    "wordFile_path = 'wordFile_path'\n",
    "\n",
    "start_date = '01-11-2018' #  09-Sep-2018 \n",
    "end_date = '30-12-2018' # 01-Oct-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will perform the following steps:\n",
    "\n",
    "#### 1. Tokenization: \n",
    "Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove small words:\n",
    "Words that have fewer than 3 characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isShortWord(token):\n",
    "    return len(token) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stopwords:\n",
    "All stopwords are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isStopWord(token):\n",
    "    return token in gensim.parsing.preprocessing.STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. lemmatized +  Stemming:\n",
    "Words are lemmatized‚Ää‚Äî‚Ääwords in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "Words are stemmed‚Ää‚Äî‚Ääwords are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token):\n",
    "    stemmer = PorterStemmer() #gensim.parsing.stem_text(tokenize) #\n",
    "    for word, tag in pos_tag(word_tokenize(token)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        lemma = WordNetLemmatizer().lemmatize(word, wntag) if wntag else word\n",
    "        return TextBlob(lemma).words[0].singularize()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Replace Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get emoji characters file path\n",
    "# def getEmojis():\n",
    "#     from dataSource import getEmojis\n",
    "#     comments_file_path = getDataSourcePathFor(emoji_path)\n",
    "#     return getEmojis(comments_file_path)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hasEmojicon(token):\n",
    "    \n",
    "# def replaceEmojicons(token, emojies):\n",
    "#     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounList(sentence='', tokens = []):\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    if len(tokens) > 0:    \n",
    "        nouns = [token for token, pos in pos_tag(tokens) if pos.startswith('NN')]\n",
    "        return nouns\n",
    "    else:\n",
    "        nouns = [token for token, pos in pos_tag(word_tokenize(sentence)) if pos.startswith('NN')]\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'I Have done reviewing, Will be seeing by them'\n",
    "# print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is file storage path\n",
    "def getDataSourcePathFor(keyForFilePath):\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    config_file_path = os.environ[virtual_env] + '/config.json'\n",
    "\n",
    "    with open(config_file_path) as f:\n",
    "        config = json.load(f)\n",
    "        if keyForFilePath in config:# ['comments_path', 'output_path']\n",
    "            return config[keyForFilePath] \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get list of comments from stored input csv file\n",
    "from dataSource import getComments, sortedMostNeg, sortedMostPos, sortedMostFreq\n",
    "def getListOfComments():\n",
    "    ### This is to get csv rows between given dates\n",
    "    comments_file_path = getDataSourcePathFor(comments_path)\n",
    "    commentsList = getComments(comments_file_path, start_date, end_date)['comments'] \n",
    "    print('Total number of comments: %s between %s and %s' % (len(commentsList), start_date, end_date))\n",
    "    return commentsList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filters(sentence):\n",
    "#     print('..given comments:', sentence)\n",
    "    result = []\n",
    "    #nouns = getNounList(sentence) # fetch only Nouns\n",
    "    for token in tokenize(sentence):#nouns: ###tokenize(text):\n",
    "        if not (isStopWord(token) or isShortWord(token)):\n",
    "            lemmaWord = lemmatize_stemming(token)\n",
    "            if not isShortWord(lemmaWord):\n",
    "                result.append(\"\".join(re.findall(\"[a-zA-Z]+\", lemmaWord)).lower())\n",
    "    return result\n",
    "\n",
    "def filterWords(tokens):\n",
    "    return list(filter(lambda each: not (isStopWord(each) or isShortWord(each)), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCommentDocument(document):\n",
    "    return list(map(lambda sentence: filters(sentence), document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordCloud import showWordCloud\n",
    "def showCloud(topicCollection):\n",
    "    showWordCloud(topicCollection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(processed_docs):\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "#     dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    return bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getSentiment import getSentiment\n",
    "\n",
    "def getSentFromCommentList(commentList):\n",
    "    sentimentList = []\n",
    "    for sentence in commentList:\n",
    "        sentiment = getSentiment(sentence)\n",
    "        sentimentList.append(sentiment)\n",
    "    return sentimentList\n",
    "\n",
    "\n",
    "def isNegative(sentiment):\n",
    "    return sentiment > 0.5\n",
    "\n",
    "\n",
    "def tokenDictWithPosNegSentiment(sentimentList, document):\n",
    "    sentDict = {}\n",
    "    size = range(len(document))\n",
    "    \n",
    "    for i in size:\n",
    "        for token in document[i]:\n",
    "            v = (0, 0, 0) # (neg, pos, freq)\n",
    "            if token in sentDict:\n",
    "                v = sentDict[token]\n",
    "                \n",
    "            if isNegative(sentimentList[i]):\n",
    "                v = (v[0]+1, v[1], v[2]+1)\n",
    "            else:\n",
    "                v = (v[0], v[1]+1, v[2]+1)\n",
    "            sentDict[token] = v\n",
    "    sentDict.pop('', None)\n",
    "    return sentDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..DBSWordList constructor called\n"
     ]
    }
   ],
   "source": [
    "from dbsWordList import DBSWordList\n",
    "file_path = getDataSourcePathFor(wordFile_path)\n",
    "trie = DBSWordList(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseToTokens(sentence):\n",
    "    #str = \"Best Bank Ever. Period. I don't know what could expect a lot of time. debit card. None of the other banks in India come close. No charges, high interests, more security, no spam msgs, reliable app, efficient customer service, so many offers, no minimum balance, truly digital with no cash business hence no useless queues n stuff, widely supported debit card, fastest transfer of funds I've ever seen...I just don't know why anyone would give it 1star...those ppl should stick with sbi or worst bank ever,axis bank...banking has never been so easier since I made an account here. Thanks for great 2yrs of service...wish it lasts way longer. üòäüòäüëçüëç\"\n",
    "    proc_doc_exist_words = trie.searchBySentence(sentence) \n",
    "    #print(proc_doc_exist_words)\n",
    "    trieFilterWords = filterWords(proc_doc_exist_words)\n",
    "    return list(set(trieFilterWords))  #getNounList('', trieFilterWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments: 972 between 01-11-2018 and 30-12-2018\n",
      "['BEST', 'free', 'app', 'good', 'BANK', 'team', 'best', 'credit', 'bank', 'digibank', 'service', 'happy']\n"
     ]
    }
   ],
   "source": [
    "commentsDocument = getListOfComments()#.head(2)\n",
    "processed_doc = commentsDocument.map(parseToTokens) #preprocessCommentDocument(commentsDocument)\n",
    "# bow_corpus = bow(processed_doc)\n",
    "print(processed_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_doc)\n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "corpus_tfidf = models.TfidfModel(bow_corpus)[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..corpos is : ['BEST', 'free', 'app', 'good', 'BANK', 'team', 'best', 'credit', 'bank', 'digibank', 'service', 'happy']\n",
      "..bow_corpus: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "Score: 0.8841338753700256\t \n",
      "Topic: 0.121*\"bank\" + 0.051*\"account\" + 0.037*\"app\" + 0.033*\"slow\" + 0.033*\"money\" + 0.029*\"add\" + 0.028*\"not able to\" + 0.027*\"login\" + 0.025*\"not working\" + 0.023*\"transfer\"\n",
      "\n",
      "Score: 0.029261434450745583\t \n",
      "Topic: 0.060*\"update\" + 0.046*\"new\" + 0.037*\"easy\" + 0.031*\"app\" + 0.028*\"password\" + 0.026*\"bank\" + 0.025*\"version\" + 0.024*\"issue\" + 0.024*\"phone\" + 0.020*\"login\"\n",
      "\n",
      "Score: 0.028961647301912308\t \n",
      "Topic: 0.073*\"experience\" + 0.057*\"best\" + 0.055*\"service\" + 0.049*\"bad\" + 0.038*\"bank\" + 0.034*\"customer\" + 0.032*\"app\" + 0.030*\"account\" + 0.023*\"slow\" + 0.020*\"good\"\n",
      "\n",
      "Score: 0.028831595554947853\t \n",
      "Topic: 0.164*\"app\" + 0.063*\"App\" + 0.036*\"dbs\" + 0.035*\"good\" + 0.032*\"worst\" + 0.025*\"great\" + 0.024*\"login\" + 0.024*\"facility\" + 0.023*\"slow\" + 0.021*\"easy\"\n",
      "\n",
      "Score: 0.02881142497062683\t \n",
      "Topic: 0.136*\"good\" + 0.100*\"app\" + 0.061*\"debit card\" + 0.053*\"option\" + 0.045*\"money\" + 0.031*\"useful\" + 0.031*\"add\" + 0.017*\"try\" + 0.016*\"Add\" + 0.014*\"transaction\"\n"
     ]
    }
   ],
   "source": [
    "# print('..corpos is :', processed_doc[0])\n",
    "# print('..bow_corpus:', bow_corpus[0])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.073*\"experience\" + 0.057*\"best\" + 0.055*\"service\" + 0.049*\"bad\"')\n",
      "(1, '0.121*\"bank\" + 0.051*\"account\" + 0.037*\"app\" + 0.033*\"slow\"')\n",
      "(2, '0.164*\"app\" + 0.063*\"App\" + 0.036*\"dbs\" + 0.035*\"good\"')\n",
      "(3, '0.060*\"update\" + 0.046*\"new\" + 0.037*\"easy\" + 0.031*\"app\"')\n",
      "(4, '0.136*\"good\" + 0.100*\"app\" + 0.061*\"debit card\" + 0.053*\"option\"')\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model_tfidf.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tokenDictWithPosNegSentiment(sentList, proc_doc_exist_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedMostPos(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from showBarGraph import *\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostPos(l), pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sortedMostNeg(l))\n",
    "# show positive bar graph\n",
    "showBarCharForSentiment(sortedMostNeg(l), pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPiChart(sortedMostFreq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showTempBarChart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWSComp",
   "language": "python",
   "name": "awscomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
